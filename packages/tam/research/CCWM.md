# **Does CCWM achieve better uncertainty calibration with fewer samples than an RSSM-style world model?**

---

# Experiment Plan: CCWM vs RSSM for Sample-Efficient Uncertainty Calibration

## 0) Hypothesis and success criteria
**Hypothesis:** For the same model capacity and data budget, CCWM learns **better-calibrated predictive uncertainty** than RSSM-style world models, especially under distribution shift / regime switching.

**Primary success criterion (calibration):**
- CCWM has lower **calibration error** (defined below) at the same number of environment steps.

**Secondary success criteria:**
- CCWM achieves similar or better **negative log likelihood** (NLL)
- CCWM achieves better **coverage–tightness tradeoff** (smaller predicted sets for same coverage)
- CCWM detects “overconfidence” episodes earlier/more reliably (optional)

---

## 1) Standardize the problem setting (keep it boring)
Use a suite of 3 environments so the result isn’t pendulum-specific:

### Env A: Your current controlled dynamics (regime-dependent actuator semantics)
- stochastic noise
- mid-episode switching (your “world flips under the bind”)

### Env B: Generic linear-Gaussian system w/ switching dynamics (easy for baselines)
- switching A,B matrices + noise
- lets you sanity-check calibration math

### Env C: Nonlinear chaotic-ish system (e.g., Duffing oscillator or simple cartpole variant)
- to stress long-horizon calibration

Keep the observation/state the same for both models (no partial observability unless you intentionally add it).

---

## 2) Define *identical* training data and budgets
### Data regime (important)
Train both models from the **same replay buffer trajectories** generated by:
- a fixed exploration policy (e.g., random torque + small stabilizing bias), OR
- your learned policy, but then you must **freeze the policy** for the calibration comparison.

For clean “sample efficiency” claims, I recommend:
- **fixed behavior policy** for data collection (so the world model is the only variable)

### Budgets (sample efficiency curve)
Pick 6–8 budgets like:
- 5k, 10k, 20k, 40k, 80k, 160k environment steps  
(or episodes, but steps are cleaner)

At each budget, train from scratch with same compute cap.

---

## 3) Models to compare (minimal, fair ablations)

### Model 1: CCWM (your commitment-conditioned tube)
- commitment z sampled once per episode: \( z \sim q(z|s_0) \), held fixed
- tube predictor gives \( \mu_{1:T}(z), \Sigma_{1:T}(z) \)
- optional halting distribution p_stop (you already have)

### Model 2: RSSM baseline (Dreamer-style)
Implement a standard RSSM with:
- deterministic hidden h_t
- stochastic latent s_t
- transition: p(s_t | h_{t-1}, a_{t-1})
- posterior: q(s_t | h_{t-1}, x_t)
- decoder predicts x_{t+1} distribution

**Match capacity**: total params within ±5–10%.

### Model 3 (ablation): “No commitment” CCWM
Same tube architecture but:
- z resampled each step (or conditioned on current state), not held fixed  
This isolates “commitment persistence” as the causal factor.

Optional:
- Model 4: RSSM + strong regularization / dropout to see if calibration is just “being conservative”

---

## 4) Evaluate the *same predictive task* for both models
Choose a prediction protocol that doesn’t privilege either.

### Protocol: multi-step predictive distributions
At random evaluation times t in held-out rollouts:
- condition on (s_t, recent history if needed)
- evaluate predictive distribution over **future states for horizons H = {1, 5, 10, 20, 40}**

For RSSM:
- sample K rollouts from the latent dynamics (K=50–200)
- estimate predictive mean/variance from samples

For CCWM:
- directly read tube mean/variance at each step (cheap)

---

## 5) Metrics (this is the heart)

### 5.1 Coverage vs nominal (calibration curve)
For each horizon h and nominal level α (e.g., 50%, 80%, 90%, 95%):
- compute predicted ellipsoid/box region from the model distribution
- measure empirical coverage on real outcomes

**Example (diagonal Gaussian, per-dimension box):**
Predicted region is:
\[
|x-\mu| \le k_\alpha \sigma
\]
Coverage error:
\[
\text{CE} = |\hat{c}_\alpha - \alpha|
\]

Aggregate:
- average across α levels and horizons

This is your core “uncertainty calibration” metric.

### 5.2 Proper scoring rule (NLL)
Compute:
- 1-step NLL and multi-step NLL (if available)
This keeps the comparison honest.

### 5.3 Sharpness / tightness
Among models with similar coverage, prefer the smaller predicted region.

Use:
- mean log(det(Σ)) per horizon, or for diagonal: mean log(σ1 σ2)
- or your cone volume proxy

### 5.4 Overconfidence rate (important for your story)
Define “overconfident miss”:
- event where outcome falls outside 90% region (or k-sigma tube)  
Then compute rate vs horizon.

This is intuitive and TAM-aligned.

### 5.5 Reliability under regime shift (separate reporting)
Split eval rollouts into:
- stable regime segments
- segments containing mid-episode switch

Report calibration separately. This is where CCWM should shine.

---

## 6) Experimental procedure (repeatable checklist)

1. **Generate held-out evaluation rollouts** (fixed seeds)
2. For each budget B:
   - collect B steps of training data with fixed behavior policy
   - train CCWM from scratch to convergence (or fixed updates)
   - train RSSM from scratch to convergence (same update count, batch size)
3. Evaluate both on the same held-out rollouts:
   - horizons H
   - nominal levels α
   - metrics above
4. Repeat with **N seeds** (at least 5; 10 if cheap)
5. Plot:
   - calibration error vs environment steps (sample efficiency curve)
   - sharpness vs coverage (Pareto frontier)
   - NLL vs steps
   - overconfident miss rate vs horizon

---

## 7) Controls that prevent “gotcha” criticism
- **Parameter-matched**: equalize #params
- **Compute-matched**: equalize gradient steps / wallclock
- **Data-matched**: same replay buffer or same collection seeds
- **Same distribution form**: both output Gaussians (RSSM via sampling approximation)

Also log:
- number of training updates
- effective batch size
- learning rate schedules

---

## 8) What results would convincingly support CCWM
You want to show either:

### A) Better calibration at equal NLL
“CCWM reduces coverage error by X% at the same predictive NLL.”

or

### B) Better calibration *and* better sharpness
“CCWM yields tighter prediction sets at equal empirical coverage.”

Even stronger:
- under regime switches, RSSM tends to become overconfident for some horizons; CCWM maintains target reliability.

---

## 9) Implementation notes (practical)
- For RSSM multi-step predictive variance, don’t rely on closed form; just sample K rollouts and compute empirical moments.
- Make sure both models output uncertainty in the same space (state space).
- If CCWM uses the dual controller to enforce bind-rate targets, give RSSM a comparable calibration mechanism in an ablation (e.g., temperature scaling of σ) so you can say the gain is not “just tuning conservativeness,” it’s the *commitment + constraint* mechanism.

---

## 10) Deliverables (what you’ll produce)
1. **Main plot:** Calibration error vs samples (multiple horizons)
2. **Secondary plot:** Sharpness vs coverage (Pareto)
3. **Shift plot:** Calibration error split by “switch happened vs not”
4. Table: NLL, overconfidence rate, and region volume at each budget
